{
  
    
        "post0": {
            "title": "Deploying web apps with Streamlit, Docker, and AWS - part 2",
            "content": "Link to part 1 here! . Deploying your web app to the cloud with AWS . Now, we will walk through how to deploy your web app to the cloud and make it publicly available! . EC2 Set up . We’ll start by heading over to aws.amazon.com/console. If you do not yet have an account, create one! After you’re logged in, locate the Services tab in the upper left-hand corner, then select EC2. . . Next, you’ll want to use the tab on the left hand-side of the console to hit Instances -&gt; Launch Instance. This will lead you to a screen prompting you to “Choose you an Amazon Machine Image”. There are many options to choose from here, but our life will be made simplest by choosing the Deep Learning AMI (Ubuntu 16.04) AMI. Using this image does introduce a bit of unneccessary overhead, however, it gurantees us that git and Docker will be pre-installed, so it will be our choice. . . look at rest of images I saved to get gist . Getting your web app in the cloud . once your instance has launched, follow the directions given here . . Then clone your webapp. For example, after sshing into the instance, mine looks like . ubuntu@ip-172-31-10-243:~$ git clone https://github.com/collinprather/streamlit-docker.git . then build and run your image on the instance . cd streamlit-docker/ docker image build -t streamlit:app . docker container run -p 8501:8501 -d streamlit:app . then, you can get your EC2 instance’s public IP address from the console, or simply type . ubuntu@ip-172-31-10-243:~/streamlit-docker$ dig +short myip.opendns.com @resolver1.opendns.com 34.220.127.248 . then go to 34.220.127.248:8501 to see your app! .",
            "url": "https://collinprather.github.io/blog/docker/aws/2020/03/11/streamlit-docker-pt2.html",
            "relUrl": "/docker/aws/2020/03/11/streamlit-docker-pt2.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deploying web apps with Streamlit, Docker, and AWS - part 1",
            "content": "Containerizing a Streamlit web app with Docker . In this tutorial, we will be assuming that you already have a streamlit app ready to deploy. If you don’t, no worries! The streamlit docs have some great tutorials, but if you’d rather jump right in, you can go ahead and git clone my small example here. . So let’s say that you’ve got your streamlit web app prepared in a directory that looks as follows: . ├── app.py # streamlit code ├── content.py # textual content - imported into app ├── images # various images for your app │   ├── logo.png │   ├── background.png ├── requirements.txt # libraries used by your app . In order to containerize this application with Docker, the first step will be to add a Dockerfile to the root of your directory. Your Dockerfile acts as a set of instructions (more specifically, a set of commands that could equivalently be called from the command line) from which Docker will build an image for your app. Using this image, Docker will then create a container. If this is all new, I’d recommend taking a look at this Docker overview! . The Dockerfile . The Dockerfile for my small example looks like this: . # base image # a little overkill but need it to install dot cli for dtreeviz FROM ubuntu:18.04 # ubuntu installing - python, pip, graphviz RUN apt-get update &amp;&amp; apt-get install python3.7 -y &amp;&amp; apt-get install python3-pip -y &amp;&amp; apt-get install graphviz -y # exposing default port for streamlit EXPOSE 8501 # making directory of app WORKDIR /streamlit-docker # copy over requirements COPY requirements.txt ./requirements.txt # install pip then packages RUN pip3 install -r requirements.txt # copying all files over COPY . . # cmd to launch app when container is run CMD streamlit run app.py # streamlit-specific commands for config ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8 RUN mkdir -p /root/.streamlit RUN bash -c &#39;echo -e &quot; [general] n email = &quot; &quot; n &quot; &gt; /root/.streamlit/credentials.toml&#39; RUN bash -c &#39;echo -e &quot; [server] n enableCORS = false n &quot; &gt; /root/.streamlit/config.toml&#39; . . The `ubuntu` portions are a bit overkill! It is worth mentioning that the ubuntu portions of this Dockerfile are a little bit overkill for the scale of this small web app, however, I found them necessary to get a nice rendering of an svg file generated by the dtreeviz package. This is also a great example of a simpler Dockerfile on this blog. There is a lot to unpack here, so I’ll do so line by line. . Ubuntu commands . At the top, we build off the base ubuntu image with the following line: . FROM ubuntu:18.04 . This means that docker pulls the ubuntu:18.04 image from dockerhub to begin with. . Next, we update and install a few things we’ll need for our web app. . RUN apt-get update &amp;&amp; apt-get install python3.7 -y &amp;&amp; apt-get install python3-pip -y &amp;&amp; apt-get install graphviz -y . Setting up the app . After that, we set up our actual application within the image. Since streamlit’s default port is 8501, we open up that port in the image. . EXPOSE 8501 . From there, we (optionally) define a working directory within the image and copy over all of our files, then install the necessary python libraries (as defined in our requirements.txt) . WORKDIR /streamlit-docker COPY requirements.txt ./requirements.txt RUN pip3 install -r requirements.txt COPY . . . . It is typically not recommended to copy all files the the images the way we&#39;ve done above (particularly is you have large files. Since this is a small example, it won&#39;t cause any issues for us. Streamlit-specific commands . Finally, we must include a few commands to ensure that streamlist runs as expected. We define a command to launch our web app whenever our docker container gets launched, . CMD streamlit run app.py . and we finish by including a few commands to configure streamlit correctly. . ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8 RUN mkdir -p /root/.streamlit RUN bash -c &#39;echo -e &quot; [general] n email = &quot; &quot; n &quot; &gt; /root/.streamlit/credentials.toml&#39; RUN bash -c &#39;echo -e &quot; [server] n enableCORS = false n &quot; &gt; /root/.streamlit/config.toml&#39; . Building the docker image and running the container . Now that we have our web app and our Dockerfile all set up, we’re ready to build the image. We can do so with a single command. . docker image build -t streamlit:app . . . Note: you must run this command from the same directory as your Dockerfile where -t stands for “tag” and . references the directory with the Dockerfile. When you run from the command line, you will see Docker moving through each step and installing lots of packages to the image. Once it is finished (make take a few minutes the first time), you should see a verification like Successfully tagged streamlit:app, letting you know that the Docker image was successfully created. You can further verify that the image was created correctly by running docker image ls, where you should see something like . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE streamlit app ecda3493de33 50 seconds ago 1.52GB . At this point, our image has been successfully built and we are ready to run the it, by way of container! (If the differences between an image and container are confusing, this short post provides some helpful distinctions.) One command will do the trick, . docker container run -p 8501:8501 -d streamlit:app . where -p allows you to publish a container’s port to the host’s port and -d allows you to run it in the background. You can then verify that is is running with a command like this, . $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82ecab4abfb1 streamlit:app &quot;/bin/sh -c &#39;streaml…&quot; 22 seconds ago Up 21 seconds 0.0.0.0:8501-&gt;8501/tcp weird_name . Better yet, pop open a web browser and you can view your web app, running in a docker container, at http://localhost:8501/. If you’re using my example, it should look something like this! . .",
            "url": "https://collinprather.github.io/blog/docker/aws/2020/03/10/streamlit-docker-pt1.html",
            "relUrl": "/docker/aws/2020/03/10/streamlit-docker-pt1.html",
            "date": " • Mar 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://collinprather.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Eigendecomposition and SVD for Deep Learning",
            "content": "I’ve been reading through Goodfellow’s Deep Learning Book, which starts with a review of the applied mathematics and machine learning basics necessary to understand deep learning. While reading through the linear algebra chapter, I realized that I had become a bit rusty working with eigenvalues and matrix decomposition, so I’ve decided to explain them here! It’s been a great exercise to review these topics myself and I hope that you find them helpful as well. . Eigendecomposition . We are often concerned with breaking mathematical objects down into smaller pieces in order to gain a better understanding of its characteristics. A classic example of this is decomposing an integer into its prime factors. For example, $60=2^2 times 3 times 5$, which tells us that 60 is divisible by 4, but not by 8 - not necessarily obvious just by looking at the number 60. In the same way, we can decompose matrices into different representations that give us some quick insight to its structure. A common form of matrix decomposition is called eigenvalue decomposition. Before we get too far, let’s define exactly what eigenvectors and eigenvalues are. . An eigenvector of a square matrix $ mathbf{A}$ is a nonzero vector $ mathbf{v}$ such that multipication by $ mathbf{A}$ alters only the scale of $ mathbf{v}$: . Av=λv mathbf{A} mathbf{v}= lambda mathbf{v}Av=λv . The scalar $ lambda$ is known as the eigenvalue corresponding to the eigenvector. . Finding eigenvalues . With a bit of algebraic manipulation, we can see that . Av=λvAv−λv=0(A−λI)v=0 begin{aligned} mathbf{A} mathbf{v}&amp;= lambda mathbf{v} mathbf{A} mathbf{v}- lambda mathbf{v}&amp;=0 left( mathbf{A}- lambda I right) mathbf{v}&amp;=0 end{aligned}AvAv−λv(A−λI)v​=λv=0=0​ . This form of the equation is useful to us because it is known that if a matrix is non-invertible, then the determinant of that matrix must equal zero. Hence, if we solve the equation . det⁡(A−λI)=0 det left( mathbf{A}- lambda I right)=0det(A−λI)=0 . (this is called the characteristic equation) for $ lambda$, then we will find all eigenvalues which satisfy $ left( mathbf{A}- lambda I right) mathbf{v}=0$. Solving this equation can sometimes be tricky, requiring polynomial long-division in order to do so by hand. . Deriving the eigendecomposition matrix . We can decompose the matrix $ mathbf{A}$ into its eigenvalues and eigenvectors with the following equation: . A=Vdiag(λ)V−1 mathbf{A}= mathbf{V} text{diag($ lambda$)} mathbf{V}^{-1}A=Vdiag(λ)V−1 . Now we will show how we can derive this equation. Suppose that we have an $k times k$ matrix $ mathbf{A}$ with eigenvectors $ mathbf{v}_1, mathbf{v}_2, dots, mathbf{v}_k$, and corresponding eigenvalues $ lambda_1, lambda_2, dots, lambda_k$. We define the matrix $ mathbf{V}$ by concatenating all of our eigenvectors into a matrix like so: . V=[v1v2…vk]=[v1,1v2,1…vk1v1,2v2,2…vk,2⋮⋮⋱⋮v1,k……vk,k] begin{aligned} mathbf{V}&amp;= left[ begin{matrix} mathbf{v}_1&amp; mathbf{v}_2&amp; dots&amp; mathbf{v}_k end{matrix} right] &amp;= left[ begin{matrix} v_{1,1}&amp; v_{2,1}&amp; dots &amp; v_{k_1} v_{1,2}&amp; v_{2,2}&amp; dots&amp; v_{k,2} vdots&amp; vdots&amp; ddots&amp; vdots v_{1,k}&amp; dots&amp; dots&amp; v_{k,k} end{matrix} right] end{aligned}V​=[v1​​v2​​…​vk​​]=⎣⎢⎢⎢⎢⎡​v1,1​v1,2​⋮v1,k​​v2,1​v2,2​⋮…​……⋱…​vk1​​vk,2​⋮vk,k​​⎦⎥⎥⎥⎥⎤​​ . We now define the $k times k$ matrix $ text{diag($ lambda$)}$ as . diag(λ)=[λ10…00λ2…0⋮⋮⋱⋮0……λk] begin{aligned} text{diag($ lambda$)}&amp;= left[ begin{matrix} lambda_1&amp;0&amp; dots &amp; 0 0&amp; lambda_2&amp; dots&amp; 0 vdots&amp; vdots&amp; ddots&amp; vdots 0&amp; dots&amp; dots&amp; lambda_k end{matrix} right] end{aligned}diag(λ)​=⎣⎢⎢⎢⎢⎡​λ1​0⋮0​0λ2​⋮…​……⋱…​00⋮λk​​⎦⎥⎥⎥⎥⎤​​ . With all of these pieces, we can see how to decompose the matrix. . AV=[Av1Av2…Avk]AV=[λ1v1λ2v2…λkvk]AV=[λ1v1,1λ2v2,1…λkvk1λ1v1,2λ2v2,2…λkvk,2⋮⋮⋱⋮λ1v1,k……λkvk,k]AV=[v1,1v2,1…vk1v1,2v2,2…vk,2⋮⋮⋱⋮v1,k……vk,k][λ10…00λ2…0⋮⋮⋱⋮0……λk]AV=Vdiag(λ)A=Vdiag(λ)V−1 begin{aligned} mathbf{A} mathbf{V}&amp;= left[ begin{matrix} mathbf{A} mathbf{v}_1&amp; mathbf{A} mathbf{v}_2&amp; dots&amp; mathbf{A} mathbf{v}_k end{matrix} right] mathbf{A} mathbf{V}&amp;= left[ begin{matrix} lambda_1 mathbf{v}_1&amp; lambda_2 mathbf{v}_2&amp; dots&amp; lambda_k mathbf{v}_k end{matrix} right] mathbf{A} mathbf{V}&amp;= left[ begin{matrix} lambda_1v_{1,1}&amp; lambda_2 v_{2,1}&amp; dots &amp; lambda_k v_{k_1} lambda_1v_{1,2}&amp; lambda_2v_{2,2}&amp; dots&amp; lambda_k v_{k,2} vdots&amp; vdots&amp; ddots&amp; vdots lambda_1 v_{1,k}&amp; dots&amp; dots&amp; lambda_k v_{k,k} end{matrix} right] mathbf{A} mathbf{V}&amp;= left[ begin{matrix} v_{1,1}&amp; v_{2,1}&amp; dots &amp; v_{k_1} v_{1,2}&amp; v_{2,2}&amp; dots&amp; v_{k,2} vdots&amp; vdots&amp; ddots&amp; vdots v_{1,k}&amp; dots&amp; dots&amp; v_{k,k} end{matrix} right] left[ begin{matrix} lambda_1&amp;0&amp; dots &amp; 0 0&amp; lambda_2&amp; dots&amp; 0 vdots&amp; vdots&amp; ddots&amp; vdots 0&amp; dots&amp; dots&amp; lambda_k end{matrix} right] mathbf{A} mathbf{V}&amp;= mathbf{V} text{diag($ lambda$)} mathbf{A}&amp;= mathbf{V} text{diag($ lambda$)} mathbf{V}^{-1} end{aligned}AVAVAVAVAVA​=[Av1​​Av2​​…​Avk​​]=[λ1​v1​​λ2​v2​​…​λk​vk​​]=⎣⎢⎢⎢⎢⎡​λ1​v1,1​λ1​v1,2​⋮λ1​v1,k​​λ2​v2,1​λ2​v2,2​⋮…​……⋱…​λk​vk1​​λk​vk,2​⋮λk​vk,k​​⎦⎥⎥⎥⎥⎤​=⎣⎢⎢⎢⎢⎡​v1,1​v1,2​⋮v1,k​​v2,1​v2,2​⋮…​……⋱…​vk1​​vk,2​⋮vk,k​​⎦⎥⎥⎥⎥⎤​⎣⎢⎢⎢⎢⎡​λ1​0⋮0​0λ2​⋮…​……⋱…​00⋮λk​​⎦⎥⎥⎥⎥⎤​=Vdiag(λ)=Vdiag(λ)V−1​ . This decomposition allows us to analyze certain properties of the matrix. For example, we can conclude that the matrix is singular if and only if any of the eigenvalues are zero. The benefits of this kind of decomposition, however, are limited. The glaring issue is that the eigendecomposition of a matrix is only defined if the matrix is square. For this reason, in practice, we usually resort to singular value decomposition instead, which is defined on all real matrices and gives us the same kind of information about the matrix. . Resources . In the coming months, I hope to learn more about the applications of matrix decomposition in the context of Deep Learning. My understanding is that we often desire to decompose weights matrices associated with neural networks to analyze how a model is learning. This paper delineates the usefulness of SVD in practice. Additionally, Charles Martin writes on the analysis of weights matrix eigenvalues in this accessible article. Finally, if you are looking for a different perspective matrix decomposition, I recommend hadrienj’s series of articles on linear algebra for Deep Learning which follow along with Goodfellow’s book. .",
            "url": "https://collinprather.github.io/blog/linear%20algebra/2018/12/31/evd-svd.html",
            "relUrl": "/linear%20algebra/2018/12/31/evd-svd.html",
            "date": " • Dec 31, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Logistic Regression and MLE",
            "content": "When traditionally using logistic regression to perform binary classification of a dataset, we make predictions, $ hat{y}$ about the class of each data point by . y^=σ(wTx+b), hat{y}= sigma(w^Tx+b),y^​=σ(wTx+b), . where $ sigma(z)= frac{1}{1+e^{-z}}$. We interpret our prediction $ hat{y}$ as the probability that the given data point is from class 1. Mathematically, $ hat{y}=p(y=1|x)$. . if $y=1$: $p(y|x)= hat{y}$ | if $y=0$: $p(y|x)=1- hat{y}$ | . We can shrink all of this math into a succint one-liner as follows: . p(y∣x)=y^y(1−y^)1−yp(y|x)= hat{y}^y(1- hat{y})^{1-y}p(y∣x)=y^​y(1−y^​)1−y . Since the log function is monotonically increasing, we can be sure that taking the log of each side of this equation holds the equality. Taking the log of each side, we see that, . log⁡(p(y∣x))=log⁡(y^y(1−y^)1−y)=ylog⁡(y^)+(1−y)log⁡(1−y^) begin{aligned} log(p(y|x))&amp;= log( hat{y}^y(1- hat{y})^{1-y}) &amp;= y log( hat{y}) + (1-y) log(1- hat{y}) end{aligned}log(p(y∣x))​=log(y^​y(1−y^​)1−y)=ylog(y^​)+(1−y)log(1−y^​)​ . If the algebra here was confusing, check out the exponent rules. Interestingly, this is precisely the log loss function that is used in logistic regression. . Under the assumption that our data points are identically, independently distributed (iid), then minimizing the log loss function over the entire data set is equivalent to performing maximum likelihood estimation of the parameters. . log p(y)=log∏i=1mp(y(i)∣x(i))log p(y)=log prod_{i=1}^{m}p(y^{(i)}|x^{(i)})log p(y)=logi=1∏m​p(y(i)∣x(i)) . Since the log of a product is equal to the sum of logs, . log⁡ p(y)=∑i=1mlog⁡ p(y(i)∣x(i))=∑i=1my(i)log⁡(y^(i))+(1−y(i))log⁡(1−y^(i))as shown above begin{aligned} log p(y) &amp;= sum_{i=1}^{m} log p(y^{(i)}|x^{(i)}) &amp;= sum_{i=1}^{m} y^{(i)} log( hat{y}^{(i)}) + (1-y^{(i)}) log(1- hat{y}^{(i)})&amp;&amp; text{as shown above} end{aligned}log p(y)​=i=1∑m​log p(y(i)∣x(i))=i=1∑m​y(i)log(y^​(i))+(1−y(i))log(1−y^​(i))​​as shown above​ . If you are familiar with machine learning, you’ll notice that this is the same as the typical logistic regression cost function, which is usually represented as so . J(w,b)=−1m∑i=1m[y(i)log⁡(y^(i)+(1−y(i))log⁡(1−y^(i))] begin{aligned} J(w,b)&amp;=- frac{1}{m} sum_{i=1}^{m} left[ y^{(i)} log( hat{y}^{(i)} + (1-y^{(i)}) log(1- hat{y}^{(i)}) right] end{aligned}J(w,b)​=−m1​i=1∑m​[y(i)log(y^​(i)+(1−y(i))log(1−y^​(i))]​ . Thus, we’ve shown that using gradient descent to identify the parameters that minimize the cost function is mathematically equivalent to performing a maximum-likelihood estimation of the parameters! .",
            "url": "https://collinprather.github.io/blog/statistics/2018/12/24/logistic-regression-and-MLE.html",
            "relUrl": "/statistics/2018/12/24/logistic-regression-and-MLE.html",
            "date": " • Dec 24, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://collinprather.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}
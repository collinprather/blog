{
  
    
        "post0": {
            "title": "Distributed Gradient Descent with Spark",
            "content": "This post will walk through how to implement the batch gradient descent algorithm in a distributed fashion, using PySpark. There are many variations of gradient descent out there, but this we will address &quot;vanilla&quot; gradient descent, and focus primarily on the implementational details in Spark. . Gradient descent is an optimization algorithm. Most typically, you&#39;ll see it associated with machine learning, which is the context we&#39;ll be working in, but it&#39;s important to acknowledge that it&#39;s fully able to stand up on it&#39;s own. The algorithm can equivalently be used to optimize a neural network or find the minimum of $f(x)=x^3-2x^2+2$. Generally speaking, the gradient descent algorithm tries to approximate what inputs correspond to the minimum value of a function. It makes no guarantees - meaning that the approximation may or may not be the true minimum. Mechanically, the algorithm exploits the property that the gradient points in the direction of steepest ascent. I will not be getting too deep into the weeds with the math, but if you&#39;re interested, here is a high-level and low-level overview. . Gradient descent with NumPy . Before jumping directly into a distributed setting, it&#39;s helpful to start with a small example. First, we&#39;ll get our head around a vectorized implementation of gradient descent for linear regression, using NumPy. . Linear regression can be used to make predictions about a continuous target variable, using a set of predictor variables. We&#39;ll refer to those predictions as $ hat{y}$, where . $$ hat{y} = underbrace{X}_{m times n} cdot underbrace{w}_{n times 1}$$and $X$ is a matrix of $m$ observations, each observation composed of $n$ predictor variables. Additionally, $w$ is a vector of $n$ weights (aka coefficients). Since our focus in on the algorithm, we&#39;ll use sklearn.datasets.make_regression() to generate a dataset, $X$, that is amenable to linear regression. . #collapse import numpy as np import matplotlib matplotlib.rcParams[&#39;text.usetex&#39;] = True import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.metrics import r2_score import time . . data_kwargs = {&quot;n_samples&quot;:100, &quot;n_features&quot;:2, &quot;noise&quot;:3, &quot;coef&quot;:True, &quot;random_state&quot;:742} X, y, coef = make_regression(**data_kwargs) w = np.zeros(X.shape[1]).reshape(-1,1) print(f&quot;X is of type: {type(X)}, and of dimension: {X.shape}.&quot;) print(f&quot;w is of type: {type(w)}, and of dimension: {w.shape}.&quot;) print(f&quot;y is of type: {type(y)}, and of dimension: {y.reshape(-1,1).shape}.&quot;) . X is of type: &lt;class &#39;numpy.ndarray&#39;&gt;, and of dimension: (100, 2). w is of type: &lt;class &#39;numpy.ndarray&#39;&gt;, and of dimension: (2, 1). y is of type: &lt;class &#39;numpy.ndarray&#39;&gt;, and of dimension: (100, 1). . . Note: linear regression often has an intercept term in addition to the weights. We choost to omit it in this post. . #collapse plt.scatter(X[:, 0], y, alpha=0.75, s=60) plt.xlabel(r&quot;$x_1$&quot;, size=20) plt.ylabel(r&quot;$y$&quot;, size=20) plt.show() . . For our case, it&#39;s safe to instantiate $w$ with zeros. In machine learning, gradient descent iteratively updates the weights to minimize some loss function. The loss function&#39;s job is to tell us how wrong our weights are at any given step of the algorithm. For linear regression, our loss function is the mean squared error of our predictions, $ hat{y}$, and the target variable, $y$. . $$ begin{aligned} MSE(y, hat{y}) &amp;= frac{1}{m} left(y - hat{y} right)^2 &amp;= frac{1}{m} left(y - X cdot w right)^2 end{aligned} $$The vectorized gradient of MSE is (for a full derivation, see this stack exchange), . $$ frac{ partial text{MSE}}{ partial w} = - left( frac{2}{m} right)X^T cdot left(y - X cdot w right)$$ You should note: in the equations about, we are doing element-wise subtraction between vectors $y$ and $ hat{y}$, as well as a dot product between a $X$ and $w$. . In code, the MSE and its gradient can be implemented like this, . def mse(X, y, w): return ((y - np.dot(X, w))**2).mean() def mse_gradient(X, y, w): residual = y - np.dot(X, w) return (-2/X.shape[0]) * np.dot(X.T, residual) . At this point, we have all the pieces we need to put the gradient descent algorithm to use in finding the optimal $w$. In our implementation, we will make n_iters updates to w, each time re-assigning it with the following rule, . $$w leftarrow w - alpha left( frac{ partial text{MSE}}{ partial w} right)$$where $ alpha$ is a small constant referred to as the learning rate. . # leaning towards verbosity to make code as clear as possible def gradient_descent(X, y, learning_rate=0.1, n_iters=100): w = np.zeros(X.shape[1]) for i in range(n_iters): w_gradient = mse_gradient(X, y, w) w = w - learning_rate*w_gradient return w . There are a couple ways to verify that our algorithm is working correctly. The simplest way is to plot our predictions against the raw data. . # change `n_features` for illustrative example data_kwargs.update({&quot;n_features&quot;:1}) X, y, coef = make_regression(**data_kwargs) w = gradient_descent(X, y) . #collapse plt.scatter(X[:, 0], y, alpha=0.75, s=60) xx = np.linspace(-3, 3, 100).reshape(-1, 1) yy = np.dot(xx, w) plt.plot(xx, yy, c=&#39;r&#39;, lw=4, alpha=0.6) plt.xlabel(r&quot;$x_1$&quot;, size=20) plt.ylabel(r&quot;$y$&quot;, size=20) plt.show() print(f&quot;Our gradient descent converged on w = {round(w.item(), 4)}.&quot;) print(f&quot;Scikit-Learn used w = {round(coef.item(), 4)} to generate the data.&quot;) . . Our gradient descent converged on w = 3.0048. Scikit-Learn used w = 3.0549 to generate the data. . It can be a bit trickier to visualize our predictions with multi-dimensional data, so we&#39;ll resort to evaluating $R^2$ scores, plotting the losses, and directly comparing the weights to Scikit-Learn&#39;s. . . Distributed gradient descent with PySpark . Many of the ideas presented here come from this lecture, given in the Spring of 2017 by Reza Zadeh, from Stanford. . A single CPU is enough to crunch all of the matrix computation in the simple example above, but in the real-world setting, it is not uncommon to deal with datasets that are quite a few orders of magnitude larger. In these scenarios, it becomes advantageous to use multiple computers, in unison, to carry out our computations. For this, Spark is the perfect tool. While Spark&#39;s MLlib has many off-the self algorithms, implementing our own gradient descent is a fantastic exercise to broaden our understanding of Spark and distributed computing. . import pyspark sc = pyspark.SparkContext().getOrCreate() . Let&#39;s simulate some big(ger) data by bumping up n_samples to 1 million and n_features to 10. . data_kwargs.update({&quot;n_samples&quot;:1_000_000, &quot;n_features&quot;:10}) X, y, coef = make_regression(**data_kwargs) data = np.hstack([X, y.reshape(-1,1)]).tolist() rdd = sc.parallelize(data).cache() print(f&quot;Our RDD has {rdd.getNumPartitions()} partitions.&quot;) . Our RDD has 12 partitions. . Above, we&#39;ve converted our dataset from a NumPy array to a Spark resilient, distributed dataset (RDD). It should be noted that we&#39;ve used np.hstack() to combine our target, $y$, with our data, $X$. We&#39;ll have to remember that the last value in each row of the RDD is the target. This RDD splits up our dataset row by row and distributes those rows across each of our 12 partitions. Since our rows are not all collected in one place, we no longer have the ability to vectorize the computation across the entire dataset - instead we&#39;ll employ for loops to make aggregations in parallel on each partition. When calculating the gradients, we must do so row by row. Our approach will be to map each row of the RDD to its corresponding gradient, with respect to the weights. After this transformation, each row should maintain its dimension. . def distributed_mse_gradient(rdd_row, w): x = rdd_row[:-1] y = rdd_row[-1] residual = y - sum(x_j*w_j for x_j, w_j in zip(x, w)) # dot product weights_gradient = [-x_j*residual for x_j in x] return weights_gradient . The line that computes residual looks starkly different than it did in the original mse_gradient() function. It uses a nested sum() and generator comprehension to compute the dot product between $x$ and $w$. . $$ underbrace{x cdot w}_{ text{dot product}} = sum_{j=1}^{n}x_j cdot w_j$$ Another distinction is that we&#39;ve dropped the constant, $2$, on the front of the gradient. Since it is a constant, it will only impact the magnitude and not the direction of the gradient, which is what it is of interest to us. . Moreover, batch gradient descent requires us to use the average of all gradients across the entire dataset. This means that we have to implement some functionality to combine the gradients in each partition, then combine the aggregated gradients across all partitions. In Spark&#39;s vernacular, we need to reduce the RDD to a single row. We&#39;ll start by taking a cumulative sum of all gradients, then dividing by the number of rows in the RDD, $m$, later. . # cumulative sum of gradients def cum_sum_gradients(row, next_row): return [gradient+next_gradient for gradient, next_gradient in zip(row, next_row)] . We these two helper functions, we can construct our distributed gradient descent loop. . # again, verbosity for clarity def distributed_gradient_descent(rdd, learning_rate=0.1, n_iters=100): w = np.zeros(len(rdd.first())-1).tolist() # -1 because the last value is y m = rdd.count() for i in range(n_iters): rdd_gradient = rdd.map(lambda row: distributed_mse_gradient(row, w)) .reduce(lambda row, next_row: cum_sum_gradients(row, next_row)) # scaling with m and learning rate w_gradient = [learning_rate*(w/m) for w in rdd_gradient] # updating weights w = [w_j - w_grad_j for w_j, w_grad_j in zip(w, w_gradient)] return w . Let&#39;s take a moment to understand which lines are transformations and which are actions. In order to set things up, we must instantiate $w$ and $m$, which require rdd.first() and rdd.count(), respectively. These are both actions, meaning they kick off the Spark driver and run the code immediately. After we enter the loop, we map each row of the RDD to its corresponding gradient with rdd.map(). In Spark, mapping is lazy evaluated, meaning that when this line hits the interpreter, nothing happens. Well, that&#39;s not entirely true, however, what you&#39;d expect does not necessarily happen. Instead of carrying out any mapping, Spark just adds the transformation to the RDD&#39;s lineage, making a plan for the computation without executing it. It is not until the .reduce() (which is an action) line gets run, that Spark begins executing on the RDD and subsequently maps, then reduces it. This mapping and reducing yields us the gradient of our mean-squared loss function, which we use to update $w$ n_iters times. . w = distributed_gradient_descent(rdd, n_iters=50) . #collapse w = [round(w_j, 2) for w_j in w] coef = [round(c, 2) for c in coef] print(f&quot;&quot;&quot;Here is a side-by-side comparison of our coefficients with Scikit-Learn&#39;s: distributed gradient descent -&gt; {w} Scikit-Learn&#39;s coefficients -&gt; {coef}&quot;&quot;&quot;) . . Here is a side-by-side comparison of our coefficients with Scikit-Learn&#39;s: distributed gradient descent -&gt; [21.07, 29.56, 15.26, 19.46, 0.9, 74.74, 18.99, 94.1, 49.16, 79.15] Scikit-Learn&#39;s coefficients -&gt; [21.18, 29.71, 15.33, 19.56, 0.91, 75.13, 19.09, 94.59, 49.41, 79.57] . Looks good! . Saving time and space with broadcasting . There is one subtlety in our distributed gradient descent implementation that can be further optimized. When we map each row to its gradient, we must also pass it w each time, as it needs it for the computation. By default, Spark makes a copy of w and sends it to each of the 12 partitions. If each partition were going to be making changes to the weights, this would be necessary, however, they are not! In fact, each partition could make the gradient computations with read-only access to w. As a result, we are wasting space by storing a copy of w on each partition, and wasting time by sending it to each partition on each iteration. . Spark&#39;s solution is to use a broadcast variable, which instead sends a single copy of w to each partition. We can add broadcasting to our distributed_gradient_descent() function with two extra lines. . # enhanced with broadcasting def distributed_gradient_descent(rdd, learning_rate=0.1, n_iters=100): w = np.zeros(len(rdd.first())-1).tolist() w = sc.broadcast(w) # broadcasting w m = rdd.count() for i in range(n_iters): rdd_gradient = rdd.map(lambda row: distributed_mse_gradient(row, w.value)) .reduce(lambda row, next_row: cum_sum_gradients(row, next_row)) w_gradient = [learning_rate*(w/m) for w in rdd_gradient] w = [w_j - w_grad_j for w_j, w_grad_j in zip(w.value, w_gradient)] w = sc.broadcast(w) # re-broadcasting w on each iter return w . After broadcasting w, we have to call w.value to access the underlying weights. . w = distributed_gradient_descent(rdd, n_iters=50) . # shutting down spark context sc.stop() .",
            "url": "https://collinprather.github.io/blog/data%20science/2020/03/15/distributed-gradient-descent.html",
            "relUrl": "/data%20science/2020/03/15/distributed-gradient-descent.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Customizing Pandas to do more with less",
            "content": "For the most part, Python dominates the Data Science landscape. In particular, there are a few core libraries that have providing the ground work for Python&#39;s flourishment, namely numpy, pandas, matplotlib and scikit-learn. Given their popularity, they&#39;ve become the primary tools leveraged by most working data scientists day-to-day and as a result, many have grown accustomed to common patterns that arise within these libraries&#39; api&#39;s. . Here is one example: it is second nature to many to reach for that random train/test split when faced with a machine learning problem (this comes directly from scikit-learn&#39;s docs). . import numpy as np from sklearn.model_selection import train_test_split X, y = np.arange(10).reshape((5, 2)), range(5) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) . Another example arises when performing EDA. pandas&#39; df.corr() goes so smoothly with seaborn&#39;s sns.heatmap(), the two are basically joined at the hip. . import pandas as pd import seaborn as sns df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39;, sep=&quot;;&quot;) sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1e97ec50&gt; . Wrapping Py-data patterns into DataFrames . Data Science is, ultimately, about the data. So why not add more core functionality to the pd.DataFrame itself? With Pandas&#39; custom accessors, we can add any arbitrary method to a pd.DataFrame! This is a useful technique to spend less time writing ubiquitous code and more time solving problems. . Train/Test splitting . Let&#39;s address the first example above. Typically when you are faced with a machine learning problem, you&#39;ll load your data into a pd.DataFrame. After some EDA, you&#39;ll want to split your dataset into a train and test set (well, you probably want a validation set too, but that&#39;s beside the point). In order to do so, you likely import sklearn&#39;s train_test_split and break your pd.DataFrame into an X and y, etc, etc. Instead of all that overhead, what if making a train/test split were this easy? . df = pd.read_csv(&quot;...&quot;) X_train, X_test, y_train, y_test = df.splits.train_test(y_col=&quot;target&quot;) . It can be. . The solution is to build a custom pd.DataFrame accessor. Pandas makes this super easy. All we have to do is write a small class that contains the typical train/test split functionality, then decorate it with @pd.api.extensions.register_dataframe_accessor(&quot;splits&quot;), where splits is the name of the attribute we&#39;ll add to a DataFrame. Here&#39;s some code. . from sklearn.model_selection import train_test_split @pd.api.extensions.register_dataframe_accessor(&quot;splits&quot;) class Splits: def __init__(self, df): self._df = df def train_test(self, y_col: str=&quot;target&quot;): X_cols = set(self._df.columns) - set([y_col]) X, y = self._df[X_cols], self._df[y_col] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) return X_train, X_test, y_train, y_test . df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39;, sep=&quot;;&quot;) X_train, X_test, y_train, y_test = df.splits.train_test(y_col=&quot;quality&quot;) print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) . (1071, 11) (528, 11) (1071,) (528,) . It may only be a few lines of code saved, but it can allow for a significant productivity boost.. particularly if you have to google those few lines of code every time. . But we&#39;re just getting started, there is a lot of room for customization. Imagine that you&#39;re working with an imbalanced dataset (we&#39;ll use scikit-learn&#39;s breast cancer dataset) and are looking for a quick way to perform stratified k-fold cross validation. . #collapse from sklearn.datasets import load_breast_cancer breast_cancer = load_breast_cancer() X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names) y = pd.DataFrame(breast_cancer.target, columns=[&quot;cancer&quot;]) df = pd.concat([X, y], axis=1) . . # okay so it&#39;s not that imbalanced.. but you get the point df[&quot;cancer&quot;].value_counts() . 1 357 0 212 Name: cancer, dtype: int64 . We can create another custom DataFrame accessor to help us out. This time, however, we&#39;ll make it a generator so that it mimics the behavior of sklearn.model_selection.StratifiedShuffleSplit. . . Note: below, I create an entirely new class for clarity, but it&#39;d be advised to implement both the train/test and stratified train/test split in a single class. . from sklearn.model_selection import StratifiedShuffleSplit @pd.api.extensions.register_dataframe_accessor(&quot;splits&quot;) class Splits: def __init__(self, df): self._df = df def stratified_train_test(self, y_col: str=&quot;target&quot;, n_splits=5, test_size=0.33, random_state=0): X_cols = set(self._df.columns) - set([y_col]) X, y = self._df[X_cols], self._df[y_col] sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state) for train_index, test_index in sss.split(X, y): yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index] . #collapse from sklearn.linear_model import LogisticRegression from sklearn.metrics import recall_score . . Now, we only need to load up our model and our dataset, and we can loop through each stratified fold right out of the box. Here is a use-case, evaluating Logistic Regression&#39;s cross-validated recall. . recalls = [] strat_kfold_df = df.splits.stratified_train_test(y_col=&quot;cancer&quot;) for X_train, X_test, y_train, y_test in strat_kfold_df: log_reg = LogisticRegression() log_reg.fit(X_train, y_train) y_preds = log_reg.predict(X_test) recalls.append(recall_score(y_test, y_preds)) print(f&quot;5-fold stratified cross-val mean recall: {round(np.array(recalls).mean(), 4)}&quot;) . 5-fold stratified cross-val mean recall: 0.9475 . EDA . When doing an exploratory data analysis, these &quot;Py-data patterns&quot; rear their head nearly everywhere. You are almost always going to want to look at some histograms, pairwise plots, or at least summary statistics. The Python data science toolkit does a great job of making these sort of things easily accessible, however, it&#39;s often necessary to import and stitch together functions from multiple libraries, or chain together five or six lines of plt.format_something(). Why not attach this EDA code directly to the DataFrame? . For instance, you may find yourself often reaching for the heatmap of correlations within a DataFrame (shown above). In many machine learning applications, however, you aren&#39;t really looking for the correlations between all variables, only the correlations with the target variable. We can create a new DataFrame accessor to give us precisely that. . #collapse import matplotlib.pyplot as plt import matplotlib.patches as mpatches # for forcing legend in corr_plot . . #collapse_show @pd.api.extensions.register_dataframe_accessor(&quot;eda&quot;) class EDA: def __init__(self, df): self._df = df def _corr(self, y_col): corr_df = self._df.corr(method=&quot;spearman&quot;)[[y_col]].drop(index=y_col) corr_df[&quot;abs_corr&quot;] = corr_df[y_col].abs() corr_series = corr_df.sort_values([&quot;abs_corr&quot;], ascending=False)[y_col] return corr_series def corr_plot(self, y_col=&quot;target&quot;): # set up corr_series = self._corr(y_col) colors = (corr_series&gt;0) pos_color, neg_color = &quot;blue&quot;, &quot;red&quot; pos_label = mpatches.Patch(color=pos_color, label=&quot;positive&quot;) neg_label = mpatches.Patch(color=neg_color, label=&quot;negative&quot;) # plotting fig = plt.figure() plt.bar(corr_series.index, corr_series.abs(), color=colors.replace(True, pos_color).replace(False, neg_color), alpha=0.75) plt.legend(handles=[pos_label, neg_label]) plt.ylabel(&quot;abs(corr)&quot;) plt.xlabel(&quot;features&quot;) plt.tick_params(axis=&#39;x&#39;, labelrotation=50) plt.show() . . df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39;, sep=&quot;;&quot;) df.eda.corr_plot(y_col=&quot;quality&quot;) . Even though this is a standard, nothing-special-about-it bar plot, it took about 20 lines of code. That&#39;s a lot. It&#39;s a serious bottleneck to have to search through Matplotlib&#39;s docs to remember how to rotate the ticks on the x-axis. Sure, this plot isn&#39;t perfect, but for a single line of code, there&#39;s a lot of information displayed - allowing you to quickly gain an understanding of which variables may be most predictive of your target. This type of rapid iteration is a differentiator between a seasoned and a mediocre analyst. . There are many potential attributes that could be added the DataFrame. My guess is that most data scientists make use of a handful of patterns across all their projects. If you&#39;re working with pd.DataFrame&#39;s every day, it is worth turning a few of your common programming idioms into new DataFrame attributes. Spend your time working on the the good stuff! .",
            "url": "https://collinprather.github.io/blog/data%20science/2020/03/14/pandas-accessors.html",
            "relUrl": "/data%20science/2020/03/14/pandas-accessors.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Deploying web apps with Streamlit, Docker, and AWS - part 3",
            "content": "This is part 3 of 3-part series. Make sure to read through part 1 and part 2 before you continue! . In parts 1 and 2, we covered how to build a Docker image for a Streamlit web app and how to move your code into the cloud. In this post, we will walk through how to connect other containerized services to your app. Specifically, we’ll connect to a Postgres database, but this process should hold for any other service you’d like to employ. . New features require new tools . We’ve had a running example of a bare-bones web app that could be used to deploy a machine learning model for use by non-technical employees. . . In some circumstances, we may want the users of this web app to have access not only to the predictions of this model, but also to certain subsets of the underlying data itself. In the example above, it would be nice to grant the users the ability to query specific rows from the dataset which fall into a given leaf of the decision tree. In other words, to give them snapshots of the data, like this (the example uses the famous Boston housing dataset, as provided by scikit-learn) .   CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 | 0.00632 | 18 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.09 | 1 | 296 | 15.3 | 396.9 | 4.98 | 24 | . 1 | 0.02731 | 0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.9 | 9.14 | 21.6 | . 2 | 0.02729 | 0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 | 0.03237 | 0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 | 0.06905 | 0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.9 | 5.33 | 36.2 | . The easiest way to add this feature to our web app would just be to save the dataset as part of our source code and ensure it gets included in the Docker image. This approach, however, can quickly become infeasible as our dataset gets large, or we want to make any updates to it. A more robust solution would be to connect our web app directly to a database, enabling us to detach the app and the data at will, and freely make revisions to either in tandem. . Introducing docker-compose . Docker has a fantastic tool called “docker-compose”, which allows you to easily chain together containers, and takes care of many details under the hood so that things just work. This is the perfect tool for our use-case. Below, we’ll walk through how to use it! . All the code used to add the database to our app can be found on the docker-compose+postgres branch of the repository. The beauty of Docker is that we do not have to make any structural changes to our app in order to interact with the database, only add a bit of functionality. . The first step in adding the database is creating a docker-compose.yml file in the root of our repository. Mine looks like this, . version: &quot;3.7&quot; services: db: image: &quot;postgres:12&quot; container_name: &quot;postgres&quot; ports: - &quot;5432:5432&quot; volumes: - db_data:/var/lib/postgresql/data env_file: - .env streamlit: build: context: . container_name: &quot;streamlit&quot; ports: - &quot;8501:8501&quot; volumes: db_data: . Let’s break it down piece by piece. . Configuring Postgres database . The first line, . version: &quot;3.7&quot; . indicates which version of the Compose file format that we are using. . The majority of our legwork falls under services, which is where we’ll define how we want to connect our database and app. I had some help from this blog in constructing the postgres portions correctly. . db: image: &quot;postgres:12&quot; container_name: &quot;postgres&quot; ports: - &quot;5432:5432&quot; volumes: - db_data:/var/lib/postgresql/data env_file: - .env . Here, we let Docker know that we want to use the postgres:12 image from DockerHub and refer to it as postgres. Since 5432 is the default port for postgres, we make sure to map that to the container’s outgoing 5432 port, so that it will be accessible by the app. Next, we mount our postgres database (db_data) to the location within the container that postgres stores all of its data, /var/lib/postgresql/data (this blog explains why this is the preferred method to gurantee our data is persisted). Lastly, we point the database towards a .env file which contains the username, password, and default name of our database. This file enables us to programmatically query the database without leaving our password in the source code! . . The environment file should never be pushed to Github! Persisting our data . All Docker containers are designed to be ephemeral, easily replaceable blocks that we can place together like legos. On the other hand, we want our data to persist even as our containers change. As Ranvir Singh said, . One of the most challenging tasks … is separating data from software. . As a solution to this problem, we define a single volume that Docker will mount to the postgres container each time it is re-built or re-run. . volumes: db_data: . Connecting to existing application . With our database aligned, we can add our existing streamlit app to the docker-compose.yml. . streamlit: build: context: . container_name: &quot;streamlit&quot; ports: - &quot;8501:8501&quot; . All the action commences with context: ., which indicates to Docker that the building of our streamlit image should be governed by a Dockerfile residing in the same directory as the docker-compose.yml. Other than that, we open up streamlit’s default 8501 port, just as we did in part 1. . Most commonly, you will already have a Postgres instance storing your data. Since our app uses data retrieved via api, there is one extra step we must take to get this data into the database. This can be handled by making use of a script that retrieves the data, then loads it into the database each time the app’s image is built. To do so, we must add the following line to our Dockerfile. . CMD python3 scripts/load_docker_db.py . An example can be found at ./scripts/load_docker_db.py in the example repository. . Putting it all together . Now, in the root of our repository, we have both a Dockerfile and a docker-compose.yml. With one command, we can build the image for our app, pull the Postgres image, connect them on one network, and run their containers together! . $ docker-compose up -d Creating network &quot;streamlit-docker_default&quot; with the default driver Creating streamlit ... done Creating postgres ... done . Where -d runs it all in the background. . Validating Postgres is working properly . With all of this happening in the background, it is helpful to take a few diagnostic steps to verify that things are working as expected. . A quick way to validate that Postgres is working is to peek at the logs. . $ docker logs -f postgres . . . &lt;various log messages&gt; . . 2020-03-12 13:13:56.831 UTC [1] LOG: database system is ready to accept connections . If you see a log stating that the database is ready to accept connections, you’re good to go! (control-C to exit the logs) . For further validation, you can even enter a psql shell within the Postgres container, then make a small query to test things out. . $ docker exec -it postgres psql -U &lt;username&gt; -d streamlit_db psql (12.2 (Debian 12.2-2.pgdg100+1)) Type &quot;help&quot; for help. streamlit_db=# select RM, LSTAT, PRICE streamlit_db-# from boston streamlit_db-# where RM &gt; 7 streamlit_db-# limit 2; rm | lstat | price -+-+- 7.185 | 4.03 | 34.7 7.147 | 5.33 | 36.2 (2 rows) streamlit_db=# q $ . Viewing and stopping your app . With our containers running, we can view the app in a web browser at http://localhost:8501/. If you need to share the app with others, you can use the steps covered in part 2 to deploy this app to the cloud. . . When you’re finished, you can use this command to stop and remove the containers running your app and database. . $ docker-compose down Stopping postgres ... done Stopping streamlit ... done Removing postgres ... done Removing streamlit ... done Removing network streamlit-docker_default .",
            "url": "https://collinprather.github.io/blog/docker/aws/2020/03/12/streamlit-docker-pt3.html",
            "relUrl": "/docker/aws/2020/03/12/streamlit-docker-pt3.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Deploying web apps with Streamlit, Docker, and AWS - part 2",
            "content": "This is part 2 of a 3-part series. Make sure to read through part 1 first, then part 3 next! . Deploying your web app to the cloud with AWS . Now, we will walk through how to deploy your web app to the cloud and make it publicly available! Just like in part 1, these instructions are tailored to my small example, but should generalize to any streamlit app you’ve built. . EC2 set-up . We’ll start by heading over to aws.amazon.com/console. If you do not yet have an account, create one! After you’re logged in, locate the Services tab in the upper left-hand corner, then select EC2. . . Next, you’ll want to use the tab on the left hand-side of the console to select Instances -&gt; Launch Instance. . . This will lead you to a screen prompting you to “Choose you an Amazon Machine Image”. There are many options to choose from here, but our life will be made simplest by choosing the Deep Learning AMI (Ubuntu) AMI. Using this image does introduce a bit of unneccessary overhead, however, it gurantees us that git and Docker will be pre-installed, so it will be our choice. . . After this, we will choose the type of instance to use. To ensure that we’ll have enough space to build and run our Docker image, it’s a safe (and cheap) bet to pick a t2.medium instance. . . . Your AWS account will be charged when you launch this instance. The good news is that you&#39;ll only be charged about $0.0464 per hour (as of 3/11/20). Don&#39;t forget to terminate your instance when finished! From here, you can skip all the way to step 6 in launching the instance, which is where you’ll “Configure Security Group”. By default, all ports on our EC2 instance, other than 22, are closed to the public. In order to make our streamlit app publicly available, we need to open up port 8501. We can do so by creating a custom tcp rule, as pictured below. . . With that set, you can click launch. . Lastly, we will need to ssh into the instance to get the code to run our app in the cloud. This requires a key pair. You should be prompted to choose an existing key pair or create one. If you do not have an existing one, choose “Create a new key pair”, then download it. Now you’re ready to click Launch Instances. . . Getting your web app in the cloud . At this point, your EC2 instance is being built and configured. You can follow its progress back in the AWS console. Once you see a green “running” icon next to your instance, you are able to toggle it, then click the Connect button near the top of the console. Follow AWS’s instructions (shown below) to ssh into the instance from your local terminal. . . After sshing into the instance, there are a few options to get our code into the cloud. This tutorial will assume your code is in a public github repository, however, if necessary, you can scp your code directly from your local computer to your instance. For our purposes, we’ll use git clone . ubuntu@ip-172-31-10-244:~$ git clone https://github.com/collinprather/streamlit-docker.git . Now that our code is on the instance, we can use the 2 commands featured in part 1 to build, then run the image (output removed for brevity). . ubuntu@ip-172-31-10-244:~$ cd streamlit-docker/ ubuntu@ip-172-31-10-244:~/streamlit-docker$ docker image build -t streamlit:app . ubuntu@ip-172-31-10-244:~/streamlit-docker$ docker container run -p 8501:8501 -d streamlit:app . Now, the web app will be served at &lt;EC2 public IP address&gt;:8501! The public IP address can be found under “IPv4 Public IP” in the AWS console. Once you’ve located it, pull open a web browser and verify that your app is running as expected! . . When you&#39;re done, don&#39;t forget to terminate your instance!",
            "url": "https://collinprather.github.io/blog/docker/aws/2020/03/11/streamlit-docker-pt2.html",
            "relUrl": "/docker/aws/2020/03/11/streamlit-docker-pt2.html",
            "date": " • Mar 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Deploying web apps with Streamlit, Docker, and AWS - part 1",
            "content": "This is part 1 of a 3 part series. Make sure to read through part 2 and part 3 next! . In this tutorial, we will be assuming the following: . You have a working Streamlit app ready to deploy If you don’t, no worries! The streamlit docs have some great tutorials, but if you’d rather jump right in, you can go ahead and git clone my small example here. | . | You have Docker installed | You have a working knowledge of the command line | . Containerizing a Streamlit web app with Docker . So let’s say that you’ve got your streamlit web app prepared in a directory that looks as follows: . ├── app.py # streamlit code ├── content.py # textual content - imported into app ├── images # various images for your app │   ├── logo.png │   ├── background.png ├── requirements.txt # libraries used by your app . In order to containerize this application with Docker, the first step will be to add a Dockerfile to the root of your directory. Your Dockerfile acts as a set of instructions (more specifically, a set of commands that could equivalently be called from the command line) from which Docker will build an image for your app. Using this image, Docker will then create a container. If this is all new, I’d recommend taking a look at this Docker overview! . The Dockerfile . The Dockerfile for my small example looks like this: . # base image # a little overkill but need it to install dot cli for dtreeviz FROM ubuntu:18.04 # ubuntu installing - python, pip, graphviz RUN apt-get update &amp;&amp; apt-get install python3.7 -y &amp;&amp; apt-get install python3-pip -y &amp;&amp; apt-get install graphviz -y # exposing default port for streamlit EXPOSE 8501 # making directory of app WORKDIR /streamlit-docker # copy over requirements COPY requirements.txt ./requirements.txt # install pip then packages RUN pip3 install -r requirements.txt # copying all files over COPY . . # cmd to launch app when container is run CMD streamlit run app.py # streamlit-specific commands for config ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8 RUN mkdir -p /root/.streamlit RUN bash -c &#39;echo -e &quot; [general] n email = &quot; &quot; n &quot; &gt; /root/.streamlit/credentials.toml&#39; RUN bash -c &#39;echo -e &quot; [server] n enableCORS = false n &quot; &gt; /root/.streamlit/config.toml&#39; . . The `ubuntu` portions are a bit overkill! It is worth mentioning that the ubuntu portions of this Dockerfile are a little bit overkill for the scale of this small web app, however, I found them necessary to get a nice rendering of an svg file generated by the dtreeviz package. This is also a great example of a simpler Dockerfile on this blog. There is a lot to unpack here, so I’ll do so line by line. . Ubuntu commands . At the top, we build off the base ubuntu image with the following line: . FROM ubuntu:18.04 . This means that Docker pulls the ubuntu:18.04 image from DockerHub to begin with. . Next, we update and install a few things we’ll need for our web app. . RUN apt-get update &amp;&amp; apt-get install python3.7 -y &amp;&amp; apt-get install python3-pip -y &amp;&amp; apt-get install graphviz -y . Setting up the app . After that, we set up our app within the image. Since streamlit’s default port is 8501, we open up that port. . EXPOSE 8501 . From there, we (optionally) define a working directory within the image and copy over all of our files, then install the necessary python libraries (as defined in our requirements.txt) . WORKDIR /streamlit-docker COPY requirements.txt ./requirements.txt RUN pip3 install -r requirements.txt COPY . . . . It is typically not recommended to copy all files to the image the way we&#39;ve done above (particularly if you have large files). However, since this is a small example, it won&#39;t cause any issues for us. Streamlit-specific commands . Finally, we must include a few commands to ensure that streamlist runs as expected. We define a command to launch our web app whenever our docker container gets launched, . CMD streamlit run app.py . and we finish by including a few commands to configure streamlit correctly. . ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8 RUN mkdir -p /root/.streamlit RUN bash -c &#39;echo -e &quot; [general] n email = &quot; &quot; n &quot; &gt; /root/.streamlit/credentials.toml&#39; RUN bash -c &#39;echo -e &quot; [server] n enableCORS = false n &quot; &gt; /root/.streamlit/config.toml&#39; . Building the docker image and running the container . Now that we have our web app and our Dockerfile all set up, we’re ready to build the image. We can do so with a single command. . docker image build -t streamlit:app . . . Note: you must run this command from the same directory as your Dockerfile where -t tags our image as streamlit:app and . references the directory with the Dockerfile. When you run from the command line, you will see Docker moving through each step defined in the Dockerfile and installing many packages to the image. Once it is finished (it may take a few minutes the first time), you should see a verification like Successfully tagged streamlit:app, letting you know that the Docker image was successfully created. You can further verify that the image was created correctly by running docker image ls, where you should see something like . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE streamlit app ecda3493de33 50 seconds ago 1.52GB . At this point, our image has been successfully built and we are ready to run it by way of container! (If the differences between an image and container are confusing, this short post provides some helpful distinctions). One command will do the trick, . $ docker container run -p 8501:8501 -d streamlit:app . where -p allows you to publish a container’s port to the host’s port and -d allows you to run it in the background. You can then verify that is is running with a command like this, . $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 82ecab4abfb1 streamlit:app &quot;/bin/sh -c &#39;streaml…&quot; 22 seconds ago Up 21 seconds 0.0.0.0:8501-&gt;8501/tcp weird_name . Better yet, pop open a web browser and you can view your web app, running in a docker container, at http://localhost:8501/. If you’re using my example, it should look something like this! . .",
            "url": "https://collinprather.github.io/blog/docker/aws/2020/03/10/streamlit-docker-pt1.html",
            "relUrl": "/docker/aws/2020/03/10/streamlit-docker-pt1.html",
            "date": " • Mar 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Eigendecomposition and SVD for Deep Learning",
            "content": "I’ve been reading through Goodfellow’s Deep Learning Book, which starts with a review of the applied mathematics and machine learning basics necessary to understand deep learning. While reading through the linear algebra chapter, I realized that I had become a bit rusty working with eigenvalues and matrix decomposition, so I’ve decided to explain them here! It’s been a great exercise to review these topics myself and I hope that you find them helpful as well. . Eigendecomposition . We are often concerned with breaking mathematical objects down into smaller pieces in order to gain a better understanding of its characteristics. A classic example of this is decomposing an integer into its prime factors. For example, $60=2^2 times 3 times 5$, which tells us that 60 is divisible by 4, but not by 8 - not necessarily obvious just by looking at the number 60. In the same way, we can decompose matrices into different representations that give us some quick insight to its structure. A common form of matrix decomposition is called eigenvalue decomposition. Before we get too far, let’s define exactly what eigenvectors and eigenvalues are. . An eigenvector of a square matrix $ mathbf{A}$ is a nonzero vector $ mathbf{v}$ such that multipication by $ mathbf{A}$ alters only the scale of $ mathbf{v}$: . Av=λv mathbf{A} mathbf{v}= lambda mathbf{v}Av=λv . The scalar $ lambda$ is known as the eigenvalue corresponding to the eigenvector. . Finding eigenvalues . With a bit of algebraic manipulation, we can see that . Av=λvAv−λv=0(A−λI)v=0 begin{aligned} mathbf{A} mathbf{v}&amp;= lambda mathbf{v} mathbf{A} mathbf{v}- lambda mathbf{v}&amp;=0 left( mathbf{A}- lambda I right) mathbf{v}&amp;=0 end{aligned}AvAv−λv(A−λI)v​=λv=0=0​ . This form of the equation is useful to us because it is known that if a matrix is non-invertible, then the determinant of that matrix must equal zero. Hence, if we solve the equation . det⁡(A−λI)=0 det left( mathbf{A}- lambda I right)=0det(A−λI)=0 . (this is called the characteristic equation) for $ lambda$, then we will find all eigenvalues which satisfy $ left( mathbf{A}- lambda I right) mathbf{v}=0$. Solving this equation can sometimes be tricky, requiring polynomial long-division in order to do so by hand. . Deriving the eigendecomposition matrix . We can decompose the matrix $ mathbf{A}$ into its eigenvalues and eigenvectors with the following equation: . A=Vdiag(λ)V−1 mathbf{A}= mathbf{V} text{diag($ lambda$)} mathbf{V}^{-1}A=Vdiag(λ)V−1 . Now we will show how we can derive this equation. Suppose that we have an $k times k$ matrix $ mathbf{A}$ with eigenvectors $ mathbf{v}_1, mathbf{v}_2, dots, mathbf{v}_k$, and corresponding eigenvalues $ lambda_1, lambda_2, dots, lambda_k$. We define the matrix $ mathbf{V}$ by concatenating all of our eigenvectors into a matrix like so: . V=[v1v2…vk]=[v1,1v2,1…vk1v1,2v2,2…vk,2⋮⋮⋱⋮v1,k……vk,k] begin{aligned} mathbf{V}&amp;= left[ begin{matrix} mathbf{v}_1&amp; mathbf{v}_2&amp; dots&amp; mathbf{v}_k end{matrix} right] &amp;= left[ begin{matrix} v_{1,1}&amp; v_{2,1}&amp; dots &amp; v_{k_1} v_{1,2}&amp; v_{2,2}&amp; dots&amp; v_{k,2} vdots&amp; vdots&amp; ddots&amp; vdots v_{1,k}&amp; dots&amp; dots&amp; v_{k,k} end{matrix} right] end{aligned}V​=[v1​​v2​​…​vk​​]=⎣⎢⎢⎢⎢⎡​v1,1​v1,2​⋮v1,k​​v2,1​v2,2​⋮…​……⋱…​vk1​​vk,2​⋮vk,k​​⎦⎥⎥⎥⎥⎤​​ . We now define the $k times k$ matrix $ text{diag($ lambda$)}$ as . diag(λ)=[λ10…00λ2…0⋮⋮⋱⋮0……λk] begin{aligned} text{diag($ lambda$)}&amp;= left[ begin{matrix} lambda_1&amp;0&amp; dots &amp; 0 0&amp; lambda_2&amp; dots&amp; 0 vdots&amp; vdots&amp; ddots&amp; vdots 0&amp; dots&amp; dots&amp; lambda_k end{matrix} right] end{aligned}diag(λ)​=⎣⎢⎢⎢⎢⎡​λ1​0⋮0​0λ2​⋮…​……⋱…​00⋮λk​​⎦⎥⎥⎥⎥⎤​​ . With all of these pieces, we can see how to decompose the matrix. . AV=[Av1Av2…Avk]AV=[λ1v1λ2v2…λkvk]AV=[λ1v1,1λ2v2,1…λkvk1λ1v1,2λ2v2,2…λkvk,2⋮⋮⋱⋮λ1v1,k……λkvk,k]AV=[v1,1v2,1…vk1v1,2v2,2…vk,2⋮⋮⋱⋮v1,k……vk,k][λ10…00λ2…0⋮⋮⋱⋮0……λk]AV=Vdiag(λ)A=Vdiag(λ)V−1 begin{aligned} mathbf{A} mathbf{V}&amp;= left[ begin{matrix} mathbf{A} mathbf{v}_1&amp; mathbf{A} mathbf{v}_2&amp; dots&amp; mathbf{A} mathbf{v}_k end{matrix} right] mathbf{A} mathbf{V}&amp;= left[ begin{matrix} lambda_1 mathbf{v}_1&amp; lambda_2 mathbf{v}_2&amp; dots&amp; lambda_k mathbf{v}_k end{matrix} right] mathbf{A} mathbf{V}&amp;= left[ begin{matrix} lambda_1v_{1,1}&amp; lambda_2 v_{2,1}&amp; dots &amp; lambda_k v_{k_1} lambda_1v_{1,2}&amp; lambda_2v_{2,2}&amp; dots&amp; lambda_k v_{k,2} vdots&amp; vdots&amp; ddots&amp; vdots lambda_1 v_{1,k}&amp; dots&amp; dots&amp; lambda_k v_{k,k} end{matrix} right] mathbf{A} mathbf{V}&amp;= left[ begin{matrix} v_{1,1}&amp; v_{2,1}&amp; dots &amp; v_{k_1} v_{1,2}&amp; v_{2,2}&amp; dots&amp; v_{k,2} vdots&amp; vdots&amp; ddots&amp; vdots v_{1,k}&amp; dots&amp; dots&amp; v_{k,k} end{matrix} right] left[ begin{matrix} lambda_1&amp;0&amp; dots &amp; 0 0&amp; lambda_2&amp; dots&amp; 0 vdots&amp; vdots&amp; ddots&amp; vdots 0&amp; dots&amp; dots&amp; lambda_k end{matrix} right] mathbf{A} mathbf{V}&amp;= mathbf{V} text{diag($ lambda$)} mathbf{A}&amp;= mathbf{V} text{diag($ lambda$)} mathbf{V}^{-1} end{aligned}AVAVAVAVAVA​=[Av1​​Av2​​…​Avk​​]=[λ1​v1​​λ2​v2​​…​λk​vk​​]=⎣⎢⎢⎢⎢⎡​λ1​v1,1​λ1​v1,2​⋮λ1​v1,k​​λ2​v2,1​λ2​v2,2​⋮…​……⋱…​λk​vk1​​λk​vk,2​⋮λk​vk,k​​⎦⎥⎥⎥⎥⎤​=⎣⎢⎢⎢⎢⎡​v1,1​v1,2​⋮v1,k​​v2,1​v2,2​⋮…​……⋱…​vk1​​vk,2​⋮vk,k​​⎦⎥⎥⎥⎥⎤​⎣⎢⎢⎢⎢⎡​λ1​0⋮0​0λ2​⋮…​……⋱…​00⋮λk​​⎦⎥⎥⎥⎥⎤​=Vdiag(λ)=Vdiag(λ)V−1​ . This decomposition allows us to analyze certain properties of the matrix. For example, we can conclude that the matrix is singular if and only if any of the eigenvalues are zero. The benefits of this kind of decomposition, however, are limited. The glaring issue is that the eigendecomposition of a matrix is only defined if the matrix is square. For this reason, in practice, we usually resort to singular value decomposition instead, which is defined on all real matrices and gives us the same kind of information about the matrix. . Resources . In the coming months, I hope to learn more about the applications of matrix decomposition in the context of Deep Learning. My understanding is that we often desire to decompose weights matrices associated with neural networks to analyze how a model is learning. This paper delineates the usefulness of SVD in practice. Additionally, Charles Martin writes on the analysis of weights matrix eigenvalues in this accessible article. Finally, if you are looking for a different perspective matrix decomposition, I recommend hadrienj’s series of articles on linear algebra for Deep Learning which follow along with Goodfellow’s book. .",
            "url": "https://collinprather.github.io/blog/linear%20algebra/2018/12/31/evd-svd.html",
            "relUrl": "/linear%20algebra/2018/12/31/evd-svd.html",
            "date": " • Dec 31, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "Logistic Regression and MLE",
            "content": "When traditionally using logistic regression to perform binary classification of a dataset, we make predictions, $ hat{y}$ about the class of each data point by . y^=σ(wTx+b), hat{y}= sigma(w^Tx+b),y^​=σ(wTx+b), . where $ sigma(z)= frac{1}{1+e^{-z}}$. We interpret our prediction $ hat{y}$ as the probability that the given data point is from class 1. Mathematically, $ hat{y}=p(y=1|x)$. . if $y=1$: $p(y|x)= hat{y}$ | if $y=0$: $p(y|x)=1- hat{y}$ | . We can shrink all of this math into a succint one-liner as follows: . p(y∣x)=y^y(1−y^)1−yp(y|x)= hat{y}^y(1- hat{y})^{1-y}p(y∣x)=y^​y(1−y^​)1−y . Since the log function is monotonically increasing, we can be sure that taking the log of each side of this equation holds the equality. Taking the log of each side, we see that, . log⁡(p(y∣x))=log⁡(y^y(1−y^)1−y)=ylog⁡(y^)+(1−y)log⁡(1−y^) begin{aligned} log(p(y|x))&amp;= log( hat{y}^y(1- hat{y})^{1-y}) &amp;= y log( hat{y}) + (1-y) log(1- hat{y}) end{aligned}log(p(y∣x))​=log(y^​y(1−y^​)1−y)=ylog(y^​)+(1−y)log(1−y^​)​ . If the algebra here was confusing, check out the exponent rules. Interestingly, this is precisely the log loss function that is used in logistic regression. . Under the assumption that our data points are identically, independently distributed (iid), then minimizing the log loss function over the entire data set is equivalent to performing maximum likelihood estimation of the parameters. . log p(y)=log∏i=1mp(y(i)∣x(i))log p(y)=log prod_{i=1}^{m}p(y^{(i)}|x^{(i)})log p(y)=logi=1∏m​p(y(i)∣x(i)) . Since the log of a product is equal to the sum of logs, . log⁡ p(y)=∑i=1mlog⁡ p(y(i)∣x(i))=∑i=1my(i)log⁡(y^(i))+(1−y(i))log⁡(1−y^(i))as shown above begin{aligned} log p(y) &amp;= sum_{i=1}^{m} log p(y^{(i)}|x^{(i)}) &amp;= sum_{i=1}^{m} y^{(i)} log( hat{y}^{(i)}) + (1-y^{(i)}) log(1- hat{y}^{(i)})&amp;&amp; text{as shown above} end{aligned}log p(y)​=i=1∑m​log p(y(i)∣x(i))=i=1∑m​y(i)log(y^​(i))+(1−y(i))log(1−y^​(i))​​as shown above​ . If you are familiar with machine learning, you’ll notice that this is the same as the typical logistic regression cost function, which is usually represented as so . J(w,b)=−1m∑i=1m[y(i)log⁡(y^(i)+(1−y(i))log⁡(1−y^(i))] begin{aligned} J(w,b)&amp;=- frac{1}{m} sum_{i=1}^{m} left[ y^{(i)} log( hat{y}^{(i)} + (1-y^{(i)}) log(1- hat{y}^{(i)}) right] end{aligned}J(w,b)​=−m1​i=1∑m​[y(i)log(y^​(i)+(1−y(i))log(1−y^​(i))]​ . Thus, we’ve shown that using gradient descent to identify the parameters that minimize the cost function is mathematically equivalent to performing a maximum-likelihood estimation of the parameters! .",
            "url": "https://collinprather.github.io/blog/statistics/2018/12/24/logistic-regression-and-MLE.html",
            "relUrl": "/statistics/2018/12/24/logistic-regression-and-MLE.html",
            "date": " • Dec 24, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". My name is Collin Prather and I am currently a Masters of Data Science student at the University of San Francisco. My background is in math and design. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://collinprather.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Featured Projects",
          "content": "Anomaly Detection with AWS Inference Pipeline . . DrivenData Turbidity Competition . . Grand Rapids Car Crash Analysis . .",
          "url": "https://collinprather.github.io/blog/featured/",
          "relUrl": "/featured/",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "Presentations/Talks",
          "content": "Machine Learning from Scratch . September 2018, 40 mins, at the Big Data Ignite Conference . I had a blast speaking at the Big Data Ignite 2018 conference. This talk dove into the math that powers machine learning. . . Deep learning for baseball card classification . April 2019, 40 mins, at Cornerstone University . . Using transfer learning to teach Keras’ MobileNetV2 to differentiate between Red Sox and Yankees baseball cards. I affectionately named the model “BambinoNet”, after the Great Bambino. . . MoneyBall is Dead . Feb 2020, 15 mins, at Standard Deviant . . For SF’s Beer Week 2020, I gave a talk titled, “MoneyBall is Dead: Why Player Development is the Final Frontier for Baseball Analytics”. This included a high-level overview of what data analysis in MLB organizations has looked like since the turn of the century and why the adoption of new high-frequency data collection devices, like high-speed cameras and radar, is changing how the game of baseball is played. . . Machine Learning Overview . February 2019, 60 mins, at SpinDance . A wide overview of the field of Machine Learning. Intended to leave you fluent enough in machine learning concepts and lingo to develop some intuition into what problems could be solved using machine learning, where it could be integrated into our current projects, and give you some direction if you decide that you’d like to learn more. . . Machine Learning for Developers . May 2019, 60 mins, at SpinDance . A technical talk walking through the steps in the ML pipeline and how developers can integrate and deploy ML solutions into their applications .",
          "url": "https://collinprather.github.io/blog/presentations/",
          "relUrl": "/presentations/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

}
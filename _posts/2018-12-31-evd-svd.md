---
title: "Eigendecomposition and SVD for Deep Learning"
toc: true
comments: true
layout: post
hide: false
search_exclude: false
description: A review of matrix decomposition that points towards applications in deep learning.
categories: [linear algebra]
---

I've been reading through Goodfellow's [Deep Learning Book](https://www.deeplearningbook.org), which starts with a review of the applied mathematics and machine learning basics necessary to understand deep learning. While reading through the linear algebra chapter, I realized that I had become a bit rusty working with eigenvalues and matrix decomposition, so I've decided to explain them here! It's been a great exercise to review these topics myself and I hope that you find them helpful as well.

# Eigendecomposition

We are often concerned with breaking mathematical objects down into smaller pieces in order to gain a better understanding of its characteristics. A classic example of this is decomposing an integer into its prime factors. For example, $60=2^2\times 3 \times 5$, which tells us that 60 is divisible by 4, but not by 8 - not necessarily obvious just by looking at the number 60. In the same way, we can decompose matrices into different representations that give us some quick insight to its structure. A common form of matrix decomposition is called **eigenvalue decomposition**. Before we get too far, let's define exactly what eigenvectors and eigenvalues are.

An **eigenvector** of a square matrix $\mathbf{A}$ is a nonzero vector $\mathbf{v}$ such that multipication by $\mathbf{A}$ alters only the scale of $\mathbf{v}$:

$$\mathbf{A}\mathbf{v}=\lambda \mathbf{v}$$

The scalar $\lambda$ is known as the **eigenvalue** corresponding to the eigenvector.

## Finding eigenvalues

With a bit of algebraic manipulation, we can see that

$$
\begin{aligned}
\mathbf{A}\mathbf{v}&=\lambda \mathbf{v}\\
\mathbf{A}\mathbf{v}-\lambda \mathbf{v}&=0\\
\left(  \mathbf{A}-\lambda I \right)\mathbf{v}&=0
\end{aligned}
$$

This form of the equation is useful to us because it is known that if a matrix is non-invertible, then the determinant of that matrix must equal zero. Hence, if we solve the equation

$$\det\left(\mathbf{A}-\lambda I\right)=0$$

(this is called the [characteristic equation](http://mathworld.wolfram.com/CharacteristicEquation.html)) for $\lambda$, then we will find all eigenvalues which satisfy $\left(  \mathbf{A}-\lambda I \right)\mathbf{v}=0$. Solving this equation can sometimes be tricky, requiring polynomial long-division in order to do so by hand.

## Deriving the eigendecomposition matrix

We can decompose the matrix $\mathbf{A}$ into its eigenvalues and eigenvectors with the following equation:

$$\mathbf{A}=\mathbf{V}\text{diag($\lambda$)}\mathbf{V}^{-1} $$

Now we will show how we can derive this equation. Suppose that we have an $k\times k$ matrix $\mathbf{A}$ with eigenvectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$, and corresponding eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_k$. We define the matrix $\mathbf{V}$ by concatenating all of our eigenvectors into a matrix like so:

$$
\begin{aligned}
\mathbf{V}&=\left[\begin{matrix}
\mathbf{v}_1& \mathbf{v}_2& \dots& \mathbf{v}_k
\end{matrix}\right]\\
&= \left[\begin{matrix}
v_{1,1}& v_{2,1}& \dots & v_{k_1}\\
v_{1,2}& v_{2,2}& \dots& v_{k,2}\\
\vdots& \vdots& \ddots& \vdots\\
v_{1,k}& \dots& \dots& v_{k,k}\\
\end{matrix}\right]
\end{aligned}
$$

We now define the $k\times k$ matrix $\text{diag($\lambda$)}$ as

$$
\begin{aligned}
\text{diag($\lambda$)}&= \left[\begin{matrix}
\lambda_1&0& \dots & 0\\
0& \lambda_2& \dots& 0\\
\vdots& \vdots& \ddots& \vdots\\
0& \dots& \dots& \lambda_k\\
\end{matrix}\right]
\end{aligned}
$$

With all of these pieces, we can see how to decompose the matrix.

$$
\begin{aligned}
\mathbf{A}\mathbf{V}&=\left[\begin{matrix}
\mathbf{A}\mathbf{v}_1& \mathbf{A}\mathbf{v}_2& \dots& \mathbf{A}\mathbf{v}_k
\end{matrix}\right]\\
\mathbf{A}\mathbf{V}&= \left[\begin{matrix}
\lambda_1\mathbf{v}_1& \lambda_2\mathbf{v}_2& \dots& \lambda_k\mathbf{v}_k
\end{matrix}\right]\\
\mathbf{A}\mathbf{V}&= \left[\begin{matrix}
\lambda_1v_{1,1}& \lambda_2 v_{2,1}& \dots &  \lambda_k v_{k_1}\\
\lambda_1v_{1,2}&  \lambda_2v_{2,2}& \dots&  \lambda_k v_{k,2}\\
\vdots& \vdots& \ddots& \vdots\\
\lambda_1 v_{1,k}& \dots& \dots&  \lambda_k v_{k,k}\\
\end{matrix}\right]\\
\mathbf{A}\mathbf{V}&= \left[\begin{matrix}
v_{1,1}& v_{2,1}& \dots & v_{k_1}\\
v_{1,2}& v_{2,2}& \dots& v_{k,2}\\
\vdots& \vdots& \ddots& \vdots\\
v_{1,k}& \dots& \dots& v_{k,k}\\
\end{matrix}\right]
\left[\begin{matrix}
\lambda_1&0& \dots & 0\\
0& \lambda_2& \dots& 0\\
\vdots& \vdots& \ddots& \vdots\\
0& \dots& \dots& \lambda_k\\
\end{matrix}\right]\\
\mathbf{A}\mathbf{V}&= \mathbf{V}\text{diag($\lambda$)}\\
\mathbf{A}&= \mathbf{V}\text{diag($\lambda$)}\mathbf{V}^{-1}
\end{aligned}
$$

This decomposition allows us to analyze certain properties of the matrix. For example, we can conclude that the matrix is singular if and only if any of the eigenvalues are zero. The benefits of this kind of decomposition, however, are limited. The glaring issue is that the eigendecomposition of a matrix is only defined if the matrix is square. For this reason, in practice, we usually resort to **[singular value decomposition](http://mathworld.wolfram.com/SingularValueDecomposition.html)** instead, which is defined on all real matrices and gives us the same kind of information about the matrix.

## Resources

In the coming months, I hope to learn more about the applications of matrix decomposition in the context of Deep Learning. My understanding is that we often desire to decompose weights matrices associated with neural networks to analyze how a model is learning. [This paper](https://arxiv.org/abs/1803.09327) delineates the usefulness of SVD in practice. Additionally, Charles Martin writes on the analysis of weights matrix eigenvalues in [this](https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/) accessible article. Finally, if you are looking for a different perspective matrix decomposition, I recommend hadrienj's series of articles on [linear algebra for Deep Learning](https://hadrienj.github.io/deep-learning-book-series-home/) which follow along with Goodfellow's book.
